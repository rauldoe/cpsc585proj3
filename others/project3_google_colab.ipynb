{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "project3_google_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rauldoe/cpsc585proj3/blob/master/project3_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoLZTkm_1iV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%tensorflow_version 2.x\n",
        "# Comment it out if not connecting to Google Drive\n",
        "from google.colab import drive\n",
        "import csv\n",
        "import timeit\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Importing the EMNIST letters\n",
        "from scipy import io as sio\n",
        "\n",
        "def checkDevice():\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "def testSpeedUp():\n",
        "  # We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "  cpu()\n",
        "  gpu()\n",
        "\n",
        "  # Run the op several times.\n",
        "  print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "        '(batch x height x width x channel). Sum of ten runs.')\n",
        "  print('CPU (s):')\n",
        "  cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "  print(cpu_time)\n",
        "  print('GPU (s):')\n",
        "  gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "  print(gpu_time)\n",
        "  print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "def connectToCloudDrive():\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "def disconnectFromCloudDrive():\n",
        "  drive.flush_and_unmount()\n",
        "\n",
        "def getCloudDriveBase():\n",
        "  return '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "def testCsv(csvPath):\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
        "    for row in reader:\n",
        "      print(', '.join(row))\n",
        "\n",
        "def testCsvOnCloudDrive(filename):\n",
        "  connectToCloudDrive()\n",
        "  filePath = getCloudDriveBase() + filename;\n",
        "  testCsv(filePath)\n",
        "  disconnectFromCloudDrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "colab": {}
      },
      "source": [
        "\n",
        "# Only run this cell to test the GPU vs CPU\n",
        "checkDevice()\n",
        "testSpeedUp()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoadHMOtFRJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Only run this cell to test the Cloud Drive\n",
        "testCsvOnCloudDrive('california_housing_test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cVMlUgU979b",
        "colab_type": "code",
        "outputId": "384bc1a6-bde9-4730-d39b-174e5ef4bfad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "filename = 'emnist-letters.mat'\n",
        "\n",
        "# Use this for local\n",
        "# filePath = './' + filename\n",
        "\n",
        "# Use this for Google Colab\n",
        "# filePath = '/content/' + filename\n",
        "\n",
        "# Use this for cloud drive\n",
        "connectToCloudDrive()\n",
        "filePath = getCloudDriveBase() + filename;\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-W0CSA-IDvT",
        "colab_type": "code",
        "outputId": "80af2d0a-205f-4578-8dab-6135140562c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# Batch size of 128 had about 90.4% accuracy.\n",
        "# Thus, a batch size of 1000 was used where accuracy was about 91.5%. \n",
        "# Signifigantly higher batch sizes also decreased test accuracy.\n",
        "# A batch size of 104,000 led to an accuracy of about \n",
        "batch_size = 2000\n",
        "# num_classes = 10\n",
        "num_classes = 26\n",
        "epochs = 1000 #There is early stopping, so it won't reach 1000 epochs. This needs to be high.\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset/53547262#53547262\n",
        "mat = sio.loadmat(filePath)\n",
        "data = mat['dataset']\n",
        "\n",
        "x_train = data['train'][0,0]['images'][0,0]\n",
        "y_train = data['train'][0,0]['labels'][0,0]\n",
        "x_test = data['test'][0,0]['images'][0,0]\n",
        "y_test = data['test'][0,0]['labels'][0,0]\n",
        "\n",
        "val_start = x_train.shape[0] - x_test.shape[0]\n",
        "x_val = x_train[val_start:x_train.shape[0],:]\n",
        "y_val = y_train[val_start:x_train.shape[0]]\n",
        "x_train = x_train[0:val_start,:]\n",
        "y_train = y_train[0:val_start]\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train - 1, num_classes, dtype='float32')\n",
        "y_test = tf.keras.utils.to_categorical(y_test - 1, num_classes, dtype='float32')\n",
        "\n",
        "y_val = tf.keras.utils.to_categorical(y_val - 1, num_classes, dtype='float32')\n",
        "\n",
        "# model = Sequential()\n",
        "# # Sigmoid seemed to work better for test accuracy compared to relu. (sigmoid was getting 91% test accuracy compared to 89% for relu.)\n",
        "# # Sigmoid was slighly better than tanh, but both were about the same test accuracy (within a few tenths of a percent)\n",
        "# model.add(Dense(512, activation='sigmoid', input_shape=(784,)))\n",
        "# # Tried different dropout rates, but 0.2 seemed to work well and provided a modest improvement.\n",
        "# # (~0.5% test accuracy improvement compared to not using dropout at all)\n",
        "# model.add(Dropout(0.2))\n",
        "# # Compared to other numbers of neurons, this number seemed to work well (2000 hidden neurons)\n",
        "# model.add(Dense(2000, activation='sigmoid'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
        "earlyStop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0.0001, patience=5, verbose=0, mode='auto',\n",
        "    baseline=None, restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs, callbacks=[earlyStop],\n",
        "                    validation_data=(x_val, y_val)\n",
        "                    )\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (104000, 28, 28, 1)\n",
            "104000 train samples\n",
            "20800 test samples\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 26)                3354      \n",
            "=================================================================\n",
            "Total params: 1,201,946\n",
            "Trainable params: 1,201,946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "52/52 [==============================] - 3s 50ms/step - loss: 1.6453 - accuracy: 0.5220 - val_loss: 77.6551 - val_accuracy: 0.7851\n",
            "Epoch 2/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.8386 - accuracy: 0.7470 - val_loss: 53.0780 - val_accuracy: 0.8644\n",
            "Epoch 3/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.6229 - accuracy: 0.8085 - val_loss: 45.9889 - val_accuracy: 0.8874\n",
            "Epoch 4/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.5240 - accuracy: 0.8384 - val_loss: 42.4316 - val_accuracy: 0.8964\n",
            "Epoch 5/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4619 - accuracy: 0.8569 - val_loss: 38.2138 - val_accuracy: 0.9082\n",
            "Epoch 6/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.4228 - accuracy: 0.8676 - val_loss: 37.0850 - val_accuracy: 0.9131\n",
            "Epoch 7/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.3846 - accuracy: 0.8796 - val_loss: 34.8662 - val_accuracy: 0.9188\n",
            "Epoch 8/1000\n",
            "52/52 [==============================] - 2s 47ms/step - loss: 0.3579 - accuracy: 0.8871 - val_loss: 38.1697 - val_accuracy: 0.9179\n",
            "Epoch 9/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.3409 - accuracy: 0.8917 - val_loss: 37.8420 - val_accuracy: 0.9177\n",
            "Epoch 10/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.3242 - accuracy: 0.8962 - val_loss: 42.7205 - val_accuracy: 0.9137\n",
            "Epoch 11/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.3055 - accuracy: 0.9016 - val_loss: 47.1366 - val_accuracy: 0.9018\n",
            "Epoch 12/1000\n",
            "52/52 [==============================] - 2s 46ms/step - loss: 0.2983 - accuracy: 0.9038 - val_loss: 44.1788 - val_accuracy: 0.9159\n",
            "Test loss: 0.26438966393470764\n",
            "Test accuracy: 0.9179326891899109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4b_oL7DAeh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finalization steps\n",
        "\n",
        "disconnectFromCloudDrive()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}