{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# %tensorflow_version 2.x\n",
    "\n",
    "import csv\n",
    "import timeit\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "# Importing the EMNIST letters\n",
    "from scipy import io as sio\n",
    "\n",
    "# from cloud import Cloud\n",
    "from hardware import Hardware\n",
    "from utility import Utility\n",
    "\n",
    "class Data2d:\n",
    "    def __init__(self, xData, yData):\n",
    "        self.x = xData\n",
    "        self.y = yData\n",
    "\n",
    "class InputShape:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.modified = None\n",
    "\n",
    "    def normalized(self):\n",
    "        return self.modified\n",
    "\n",
    "class LayerDescriptor:\n",
    "    def __init__(self, modelDescriptor, inputList, index, descriptionCount):\n",
    "        self.modelDescriptor = modelDescriptor\n",
    "        self.InputList = inputList\n",
    "        self.LayerType = inputList[0]\n",
    "        self.data = ','.join(inputList)\n",
    "        self.index = index\n",
    "        self.descriptionCount = descriptionCount\n",
    "        self.Activation = None\n",
    "        self.FilterCount = None\n",
    "        self.KernelSize = None\n",
    "\n",
    "    def build(self):\n",
    "        layerType = self.LayerType\n",
    "        inputList = self.InputList\n",
    "\n",
    "        if (layerType == 'conv2d'):\n",
    "            self.Activation = inputList[1]\n",
    "            self.FilterCount = int(inputList[2])\n",
    "            self.KernelSize = (int(inputList[3]), int(inputList[4]))\n",
    "\n",
    "            kernelRegularizer = LayerDescriptor.kernelRegularizer(inputList[3], inputList[4])\n",
    "            if (self.index == 0):\n",
    "                layer = Conv2D(self.FilterCount, kernel_size=self.KernelSize, activation=self.Activation, kernel_regularizer=kernelRegularizer, input_shape=self.modelDescriptor.inputShape.normalized())\n",
    "            else:\n",
    "                layer = Conv2D(self.FilterCount, kernel_size=self.KernelSize, activation=self.Activation, kernel_regularizer=kernelRegularizer)\n",
    "\n",
    "        elif (self.LayerType == 'maxpooling2d'):\n",
    "            layer = MaxPooling2D(pool_size=(int(inputList[1]), int(inputList[2])))\n",
    "\n",
    "        elif (self.LayerType == 'dropout'):\n",
    "            layer = Dropout(float(inputList[1]))\n",
    "\n",
    "        elif (self.LayerType == 'flatten'):\n",
    "            layer = Flatten()\n",
    "\n",
    "        elif (self.LayerType == 'dense'):\n",
    "            self.Activation = inputList[1]\n",
    "\n",
    "            if (self.index+1 == self.descriptionCount):\n",
    "                layer = Dense(self.modelDescriptor.numberOfClasses, activation=self.Activation)\n",
    "            else:\n",
    "                layer = Dense(int(inputList[2]), activation=self.Activation)\n",
    "        \n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def kernelRegularizer(funcName, value):\n",
    "\n",
    "        if (funcName == 'l1'):\n",
    "            return regularizers.l1(value)\n",
    "        elif (funcName == 'l2'):\n",
    "            return regularizers.l2(value)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def create(modelDescriptor, inputList, index, descriptionCount):\n",
    "        return LayerDescriptor(modelDescriptor, inputList, index, descriptionCount)\n",
    "\n",
    "class LayerInstance:\n",
    "    def __init__(self, layer, descriptor):\n",
    "        self.layer = layer\n",
    "        self.descriptor = descriptor\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self, train, test, val, inputShape):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.val = val\n",
    "        self.inputShape = inputShape\n",
    "\n",
    "class ModelDescriptor:\n",
    "    def __init__(self, descriptorFilepath, archivePath, badPath, hyperparameters, descriptions, inputShape, numberOfClasses, threshholdAccuracy):\n",
    "        self.descriptorFilepath = descriptorFilepath\n",
    "        self.archivePath = archivePath\n",
    "        self.badPath = badPath\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.descriptions = descriptions\n",
    "        self.threshholdAccuracy = threshholdAccuracy\n",
    "\n",
    "        # processCount, epochCount, batchSize, kernelRegularizer, kernelRegularizerValue, loss, optimizer\n",
    "        i = -1\n",
    "\n",
    "        i += 1\n",
    "        self.processCount = int(hyperparameters[i])\n",
    "        i += 1\n",
    "        self.epochCount = int(hyperparameters[i])\n",
    "        i += 1\n",
    "        self.batchSize = int(hyperparameters[i])\n",
    "        i += 1\n",
    "        self.loss = hyperparameters[i]\n",
    "        i += 1\n",
    "        self.optimizer = hyperparameters[i]\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.numberOfClasses = numberOfClasses\n",
    "        self.hash = Utility.generateHashFromFile(descriptorFilepath)\n",
    "    \n",
    "    def archive(self):\n",
    "        shutil.move(self.descriptorFilepath,  self.archivePath)\n",
    "\n",
    "    def bad(self):\n",
    "        shutil.move(self.descriptorFilepath,  self.badPath)\n",
    "\n",
    "    @staticmethod\n",
    "    def create(descriptorFilepath, archivePath, badPath, inputShape, numberOfClasses, threshholdAccuracy):\n",
    "        descriptions = Utility.readCsv(descriptorFilepath)\n",
    "        hyperparameters = descriptions.pop(0)\n",
    "        return ModelDescriptor(descriptorFilepath, archivePath, badPath, hyperparameters, descriptions, inputShape, numberOfClasses, threshholdAccuracy)\n",
    "\n",
    "class ModelInstance:\n",
    "    def __init__(self, descriptor, model=None, layers=None, score=None):\n",
    "        self.descriptor = descriptor\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.score = score\n",
    "\n",
    "    @staticmethod\n",
    "    def loadModelData(matFilepath, modelDescriptor):\n",
    "\n",
    "        inputShape = copy.deepcopy(modelDescriptor.inputShape)\n",
    "        numberOfClasses = modelDescriptor.numberOfClasses\n",
    "\n",
    "        # https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset/53547262#53547262\n",
    "        # mat = sio.loadmat('emnist-letters.mat')\n",
    "        mat = sio.loadmat(matFilepath)\n",
    "        data = mat['dataset']\n",
    "\n",
    "        train = Data2d(data['train'][0,0]['images'][0,0], data['train'][0,0]['labels'][0,0])\n",
    "        test = Data2d(data['test'][0,0]['images'][0,0], data['test'][0,0]['labels'][0,0])\n",
    "\n",
    "        trainShape = train.x.shape[0]\n",
    "        testShape = test.x.shape[0]\n",
    "\n",
    "        val_start = trainShape - testShape\n",
    "        val = Data2d(train.x[val_start:trainShape,:], train.y[val_start:trainShape])\n",
    "        train.x = train.x[0:val_start,:]\n",
    "        train.y = train.y[0:val_start]\n",
    "\n",
    "        trainShape = train.x.shape[0]\n",
    "        testShape = test.x.shape[0]\n",
    "        valShape = val.x.shape[0]\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            train.x = train.x.reshape(trainShape, 1, inputShape.rows, inputShape.cols)\n",
    "            test.x = test.x.reshape(testShape, 1, inputShape.rows, inputShape.cols)\n",
    "            val.x = val.x.reshape(valShape, 1, inputShape.rows, inputShape.cols)\n",
    "            inputShape.modified = (1, inputShape.rows, inputShape.cols)\n",
    "        else:\n",
    "            train.x = train.x.reshape(trainShape, inputShape.rows, inputShape.cols, 1)\n",
    "            test.x = test.x.reshape(testShape, inputShape.rows, inputShape.cols, 1)\n",
    "            val.x = val.x.reshape(valShape, inputShape.rows, inputShape.cols, 1)\n",
    "            inputShape.modified = (inputShape.rows, inputShape.cols, 1)\n",
    "\n",
    "        train.x = train.x.astype('float32')\n",
    "        test.x = test.x.astype('float32')\n",
    "        train.x /= 255\n",
    "        test.x /= 255\n",
    "        print('x_train shape:', train.x.shape)\n",
    "        print(train.x.shape[0], 'train samples')\n",
    "        print(test.x.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        train.y = tf.keras.utils.to_categorical(train.y - 1, numberOfClasses, dtype='float32')\n",
    "        test.y = tf.keras.utils.to_categorical(test.y - 1, numberOfClasses, dtype='float32')\n",
    "\n",
    "        val.y = tf.keras.utils.to_categorical(val.y - 1, numberOfClasses, dtype='float32')\n",
    "\n",
    "        return ModelData(train, test, val, inputShape)\n",
    "\n",
    "    @staticmethod\n",
    "    def buildLayers(modelInstance, descriptions):\n",
    "        layers = []\n",
    "\n",
    "        descriptionCount = len(descriptions)\n",
    "        for i in range(descriptionCount):\n",
    "            desc = descriptions[i]\n",
    "            layerDescriptor = LayerDescriptor.create(modelInstance.descriptor, desc, i, descriptionCount)\n",
    "            layers.append(LayerInstance(layerDescriptor.build(), layerDescriptor))\n",
    "\n",
    "        modelInstance.layers = layers\n",
    "\n",
    "        return modelInstance\n",
    "\n",
    "    @staticmethod\n",
    "    def buildModel(modelInstance, layerInstances):\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(len(layerInstances)):\n",
    "            layer = layerInstances[i].layer\n",
    "            layerDescriptor = layerInstances[i].descriptor\n",
    "            print(\"i: %d type: %s data: %s\"%(i, layerDescriptor.LayerType, layerDescriptor.data)) \n",
    "            model.add(layer)\n",
    "            print(\"i: %d added\"%(i)) \n",
    "\n",
    "        model.compile(loss=modelInstance.descriptor.loss,\n",
    "                    optimizer=modelInstance.descriptor.optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        modelInstance.model = model\n",
    "\n",
    "        return modelInstance\n",
    "\n",
    "    @staticmethod\n",
    "    def create(descriptor):\n",
    "        return ModelInstance(descriptor)\n",
    "\n",
    "class ScoreBoard:\n",
    "    def __init__(self, scoreFilepath):\n",
    "        self.scoreFilepath = scoreFilepath\n",
    "        self.scoreList = Utility.readCsv(scoreFilepath)\n",
    "    \n",
    "    def isDone(self, modelDescriptor):\n",
    "        itemList = self.getItems(modelDescriptor.hash)\n",
    "        if (len(itemList) > 0):\n",
    "            scoreObj = itemList[0]\n",
    "            \n",
    "            accuracy = float(scoreObj[1])\n",
    "            return ((accuracy < modelDescriptor.threshholdAccuracy) or (int(scoreObj[4]) >= modelDescriptor.processCount))\n",
    "\n",
    "        return False\n",
    "\n",
    "    def isInList(self, identifier):\n",
    "        return True in (i[0] == identifier for i in self.scoreList)\n",
    "    \n",
    "    def getItems(self, identifier):\n",
    "        return list(filter(lambda x: x[0] == identifier, self.scoreList))\n",
    "\n",
    "    def record(self, modelDescriptor, score):\n",
    "\n",
    "        scoreObj = None\n",
    "        count = 0\n",
    "        itemList = self.getItems(modelDescriptor.hash)\n",
    "        if (len(itemList) > 0):\n",
    "            scoreObj = itemList[0]\n",
    "            count = int(scoreObj[4]) + 1\n",
    "            scoreObj[4] = count\n",
    "            if (score[1] > float(scoreObj[1])):\n",
    "                scoreObj[1] = score[1]\n",
    "                scoreObj[2] = score[0]\n",
    "        else:\n",
    "            count = 1\n",
    "            scoreObj = [modelDescriptor.hash, score[1], score[0], modelDescriptor.descriptorFilepath, count]\n",
    "            self.scoreList.append(scoreObj)\n",
    "        \n",
    "        self.scoreList.sort(key=lambda i:float(i[1]), reverse=True)\n",
    "\n",
    "        newList = copy.copy(self.scoreList)\n",
    "        newList.insert(0, ['#hash', 'score1', 'score0', 'filePath', 'count'])\n",
    "        Utility.writeCsv(self.scoreFilepath, newList)\n",
    "\n",
    "        if (self.isDone(modelDescriptor)):\n",
    "            modelDescriptor.archive()\n",
    "\n",
    "def processNN(matFilepath, modelInstance):\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', min_delta=0.0001, patience=5, verbose=0, mode='auto',\n",
    "            baseline=None, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    modelData = ModelInstance.loadModelData(matFilepath, modelInstance.descriptor)\n",
    "\n",
    "    modelInstance.descriptor.inputShape = modelData.inputShape\n",
    "\n",
    "    modelInstance = ModelInstance.buildLayers(modelInstance, modelInstance.descriptor.descriptions)\n",
    "\n",
    "    try:\n",
    "        modelInstance = ModelInstance.buildModel(modelInstance, modelInstance.layers)\n",
    "\n",
    "        modelInstance.model.fit(modelData.train.x, modelData.train.y,\n",
    "                            batch_size=modelInstance.descriptor.batchSize,\n",
    "                            epochs=modelInstance.descriptor.epochCount, callbacks=[earlyStop],\n",
    "                            validation_data=(modelData.val.x, modelData.val.y)\n",
    "                            )\n",
    "        modelInstance.model.summary()\n",
    "        score = modelInstance.model.evaluate(modelData.test.x, modelData.test.y, verbose=0)\n",
    "\n",
    "        # score = [54.34, 98.55]\n",
    "        modelInstance.score = score\n",
    "\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "\n",
    "    return modelInstance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (104000, 28, 28, 1)\n",
      "104000 train samples\n",
      "20800 test samples\n",
      "i: 0 type: conv2d data: conv2d,relu,64,9,9,,\n",
      "i: 0 added\n",
      "i: 1 type: conv2d data: conv2d,relu,128,3,3\n",
      "i: 1 added\n",
      "i: 2 type: maxpooling2d data: maxpooling2d,7,7\n",
      "i: 2 added\n",
      "i: 3 type: dropout data: dropout,0.25\n",
      "i: 3 added\n",
      "i: 4 type: flatten data: flatten\n",
      "i: 4 added\n",
      "i: 5 type: dense data: dense,relu,400\n",
      "i: 5 added\n",
      "i: 6 type: dropout data: dropout,0.3\n",
      "i: 6 added\n",
      "i: 7 type: dense data: dense,softmax\n",
      "i: 7 added\n",
      "Train on 104000 samples, validate on 20800 samples\n",
      "Epoch 1/1000\n",
      "  2000/104000 [..............................] - ETA: 2:46 - loss: 3.2709 - accuracy: 0.0390"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-070787d39adc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmodelInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelInstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelDescriptor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mmodelInstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatFilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelInstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodelInstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f6f9eff63f1a>\u001b[0m in \u001b[0;36mprocessNN\u001b[1;34m(matFilepath, modelInstance)\u001b[0m\n\u001b[0;32m    316\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodelInstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodelInstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochCount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlyStop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m                             )\n\u001b[0;32m    320\u001b[0m         \u001b[0mmodelInstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch size of 128 had about 90.4% accuracy.\n",
    "# Thus, a batch size of 1000 was used where accuracy was about 91.5%. \n",
    "# Signifigantly higher batch sizes also decreased test accuracy.\n",
    "# A batch size of 104,000 led to an accuracy of about \n",
    "# batchSize = 2000\n",
    "# epochCount = 1000 #There is early stopping, so it won't reach 1000 epochs. This needs to be high.\n",
    "\n",
    "inputShape = InputShape(28, 28)\n",
    "numberOfClasses = 26\n",
    "\n",
    "matFilepath = './emnist-letters.mat'\n",
    "\n",
    "scoreFilepath = './models/results/scoreboard.csv'\n",
    "\n",
    "modelDir = './models/'\n",
    "archivePath = './models/archive/'\n",
    "badPath = './models/archive/bad/'\n",
    "threshholdAccuracy = 0.90\n",
    "\n",
    "scoreBoard = ScoreBoard(scoreFilepath)\n",
    "modelFiles = Utility.getFiles(modelDir)\n",
    "for modelFile in modelFiles:\n",
    "    descriptorPath = '%s%s'%(modelDir, modelFile)\n",
    "    modelDescriptor = ModelDescriptor.create(descriptorPath, archivePath, badPath, inputShape, numberOfClasses, threshholdAccuracy)\n",
    "\n",
    "    if (scoreBoard.isDone(modelDescriptor)):\n",
    "        modelDescriptor.archive()\n",
    "    else:\n",
    "        modelInstance = ModelInstance.create(modelDescriptor)\n",
    "\n",
    "        modelInstance = processNN(matFilepath, modelInstance)\n",
    "\n",
    "        if (modelInstance.score == None):\n",
    "            modelDescriptor.bad()\n",
    "        else:\n",
    "            scoreBoard.record(modelDescriptor, modelInstance.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
