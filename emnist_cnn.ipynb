{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from scipy  import io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "mat = sio.loadmat('emnist-letters.mat')\n",
    "data = mat['dataset']\n",
    "\n",
    "img_rows, img_cols = 28,28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 784)\n",
      "(20800, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = data['train'][0,0]['images'][0,0]\n",
    "Y_train = data['train'][0,0]['labels'][0,0]\n",
    "X_test = data['test'][0,0]['images'][0,0]\n",
    "Y_test = data['test'][0,0]['labels'][0,0]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(-1,784).astype('float32')\n",
    "X_test = X_test.reshape(-1,784).astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train - 1).astype('float32')\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test - 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 28, 28, 1)\n",
      "(124800, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124800\n",
      "20800\n",
      "104000\n"
     ]
    }
   ],
   "source": [
    "M = X_train.shape[0]\n",
    "\n",
    "\n",
    "M_test = X_test.shape[0]\n",
    "print(M)\n",
    "print(M_test)\n",
    "\n",
    "split_size = M - M_test\n",
    "\n",
    "print(split_size)\n",
    "\n",
    "X_train, x_cv = X_train[:split_size], X_train[split_size:]\n",
    "Y_train, y_cv = Y_train[:split_size], Y_train[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104000, 28, 28, 1)\n",
      "(20800, 28, 28, 1)\n",
      "(104000, 26)\n",
      "(20800, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(x_cv.shape)\n",
    "print(Y_train.shape)\n",
    "print(y_cv.shape)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams is a dictionary:\n",
    "# {'batch_size': 1-400, \n",
    "# 'epochs' = 10-15, \n",
    "# 'num_conv_layers' = 1-4, \n",
    "# 'num_filters': 0-64, \n",
    "# 'filter_size': 0-8, \n",
    "# 'use_max_pool': 0-1, \n",
    "# 'max_pool_size: 1-8'\n",
    "# 'num_dense_layers: 0-8' \n",
    "# 'num_dense_neurons: 1-64'\n",
    "# 'dense_regularizer: 0-1'\n",
    "# 'lambda:' 0.00001 - 0.00005'\n",
    "# 'dense_dropout_value: 0 - 0.5'\n",
    "\n",
    "import random\n",
    "\n",
    "num_classes = 26\n",
    "\n",
    "def clamp(x, small, large):\n",
    "    return min(max(x, small), large)\n",
    "\n",
    "def gaussian_int(curr_value, var, range_values):\n",
    "    return clamp( round(np.random.normal(curr_value, var)), range_values[0], range_values[1])\n",
    "\n",
    "def gaussian_float(curr_value, var, range_values):\n",
    "    return clamp( np.random.normal(curr_value, var), range_values[0], range_values[1])\n",
    "\n",
    "def mutate_keep_architecture(hp):\n",
    "    hp = hp.copy()\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['batch_size'] = gaussian_int(hp['batch_size'], 30, (100, 400))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['epochs'] = gaussian_int(hp['epochs'], 5, (10, 20))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_filters'] = gaussian_int(hp['num_filters'], 4, (28, 64))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['filter_size'] = gaussian_int(hp['filter_size'], 1, (1, 8))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['max_pool_size'] = gaussian_int(hp['max_pool_size'], 1, (0, 8))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_dense_neurons'] = gaussian_int(hp['num_dense_neurons'], 4, (1, 64))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['dense_regularizer'] = gaussian_int(hp['dense_regularizer'], 0.5, (0, 1))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['lambda'] = gaussian_float(hp['lambda'], .000001, (0.00001, 0.00005))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_dense_neurons'] = gaussian_int(hp['num_dense_neurons'], 4, (32, 400))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['dense_dropout_value'] = gaussian_float(hp['dense_dropout_value'], .01, (0, 0.5))\n",
    "    return hp\n",
    "\n",
    "def mutate(hp):\n",
    "    hp = hp.copy()\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['batch_size'] = gaussian_int(hp['batch_size'], 30, (100, 400))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['epochs'] = gaussian_int(hp['epochs'], 5, (10, 20))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_conv_layers'] = gaussian_int(hp['num_conv_layers'], 1, (1, 4))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_filters'] = gaussian_int(hp['num_filters'], 4, (28, 64))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['filter_size'] = gaussian_int(hp['filter_size'], 1, (1, 8))\n",
    "    #hp['use_max_pool'] = gaussian_int(hp['use_max_pool'], 0.5, (0, 1))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['max_pool_size'] = gaussian_int(hp['max_pool_size'], 1, (0, 8))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_dense_layers'] = gaussian_int(hp['num_dense_layers'], 1, (0, 8))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_dense_neurons'] = gaussian_int(hp['num_dense_neurons'], 4, (1, 64))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['dense_regularizer'] = gaussian_int(hp['dense_regularizer'], 0.5, (0, 1))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['lambda'] = gaussian_float(hp['lambda'], .000001, (0.00001, 0.00005))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['num_dense_neurons'] = gaussian_int(hp['num_dense_neurons'], 4, (32, 400))\n",
    "    if random.uniform() < 0.5:\n",
    "        hp['dense_dropout_value'] = gaussian_float(hp['dense_dropout_value'], .01, (0, 0.5))\n",
    "    return hp\n",
    "    \n",
    "def initialize():\n",
    "    return {'batch_size': random.choice(range(100,401)), \n",
    "    'epochs': random.choice(range(10,20)), \n",
    "    'num_conv_layers': random.choice(range(1,4)), \n",
    "    'num_filters': random.choice(range(28,128)), \n",
    "    'filter_size': random.choice(range(8)), \n",
    "    'use_max_pool': 1, # random.choice(range(2)), \n",
    "    'max_pool_size': random.choice(range(1,8)),\n",
    "    'num_dense_layers': random.choice(range(8)), \n",
    "    'num_dense_neurons': random.choice(range(32,400)),\n",
    "    'dense_regularizer' : gaussian_int(0.5, 0.5, (0, 1)),\n",
    "    'lambda': gaussian_float(0.00003, .000001, (0.00001, 0.00005)),\n",
    "    'dense_dropout_value': gaussian_float(0.3, .1, (0, 0.5)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = initialize()\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutate(hp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "\n",
    "def run_model(hp):\n",
    "    print(hp)\n",
    "    model = Sequential()\n",
    "    d = 28\n",
    "    for x in range(hp['num_conv_layers']):\n",
    "        fs = hp['filter_size']\n",
    "        d -= fs-1\n",
    "        if d > 0:\n",
    "            model.add(Conv2D(hp['num_filters'], (fs, fs), activation = 'relu'))\n",
    "            \n",
    "    \n",
    "    if hp['use_max_pool'] == 1:\n",
    "            ps = hp['max_pool_size']\n",
    "            if d - (ps-1) > 0:\n",
    "                model.add(MaxPooling2D(pool_size= (ps, ps)))\n",
    "    model.add(Flatten())\n",
    "    for x in range(hp['num_dense_layers']):\n",
    "        if hp['dense_regularizer'] == 0:\n",
    "            model.add(Dense(hp['num_dense_neurons'], activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(hp['num_dense_neurons'], activation='relu', kernel_regularizer = keras.regularizers.l2(hp['lambda'])))\n",
    "    if hp['dense_dropout_value'] > 0:\n",
    "        model.add(Dropout(hp['dense_dropout_value']))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adam(), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, Y_train,\n",
    "          batch_size=hp['batch_size'],\n",
    "          epochs=hp['epochs'],\n",
    "          verbose=1,\n",
    "          validation_data = (x_cv, y_cv))\n",
    "    \n",
    "    return model.evaluate(X_test, Y_test, verbose=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas based off of this paper http://www.ijmlc.org/vol9/874-L0279.pdf\n",
    "# Running this toasted my GPU after 2 hrs and performance slowed to a crawl\n",
    "# DO NOT RUN. Needs more work.\n",
    "\n",
    "\n",
    "def search():\n",
    "\n",
    "    score = 0\n",
    "    iterations = 1\n",
    "    select_value= 4\n",
    "    clone_value = 5\n",
    "\n",
    "    candidates = [(initialize(), score)]\n",
    "\n",
    "    while score < 0.95:\n",
    "        print('Start iteration: ', iterations)\n",
    "        iterations += 1\n",
    "        \n",
    "        # sort by test score\n",
    "        best = candidates[0][0]\n",
    "        next_gen = [best]\n",
    "        for x in range(select_value):     # take 5 mutations but keep the main architecture\n",
    "            next_gen.append(mutate_keep_architecture(best))\n",
    "        for p in next_gen:\n",
    "            hp, score = run_model(p)\n",
    "            print('Score: ', score)\n",
    "            candidates.append((p, score))\n",
    "        \n",
    "        candidates = sorted(candidates, key = lambda x: -x[1])[:4]  # sort and take the top 4\n",
    "        print(candidates)\n",
    "        \n",
    "        candidates2 = []\n",
    "        for c,s in candidates:\n",
    "            for _ in range(clone_value):\n",
    "                candidates2.append(mutate(c))   # take mutations of top 4 with same architecture and mutate arbitrarily\n",
    "        \n",
    "        for c in candidates2:\n",
    "            hp, score = run_model(c)\n",
    "            print('Score: ', score)\n",
    "            candidates.append((hp, score))\n",
    "            \n",
    "        \n",
    "        candidates = sorted(candidates, key = lambda x: -x[1])\n",
    "        score = candidates[0][1]\n",
    "        \n",
    "    return candidates[0]\n",
    "        \n",
    "\n",
    "search()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notes\n",
    "\n",
    "# batch 512, 30 epochs, 2D Conv with 5x5 kernel size and dropout around 0.25 and 0.2 seems to work best so far\n",
    "# Hit 96% in training and cross validation, but only 93-94% in test.\n",
    "\n",
    "# Try adding regularizer and changing dropout value. L2 seems to work better than L1.\n",
    "\n",
    "# Droput 0.4 Test loss: 0.29938104220307793\n",
    "# Test accuracy: 0.9275480508804321   15 epochs\n",
    " \n",
    "# Dropout 0.35 Test loss: 0.28941035776184154\n",
    "# Test accuracy: 0.9307211637496948 under Cond2D 5x5, Cond2D 5x5, 256 dense, dropout  15 epochs\n",
    "\n",
    "##############################\n",
    "# changed architecture try with more dense layers\n",
    "# seems to have higher test Cond2D 5x5, Cond2D 5x5, 3x 100 dense with dropout\n",
    "# Test loss: 0.20129575222587356\n",
    "# Test accuracy: 0.9350481033325195  15 epochs\n",
    "\n",
    "###########################\n",
    "# changed to Cond2D 7x7, Cond2D 5x5, maxpool 2, 200 dense seems to learn very fast but prone to overfitting\n",
    "# batch size 400\n",
    "# added dropout 0.2\n",
    "\n",
    "\n",
    "############################\n",
    "# Tried Cond2D 9x9 32, Cond2D 5x5 64, Cond2D 3x3 128, MaxPooling 2, \n",
    "\n",
    "# Seems a high kernel_filter works better. Below is the best hand tuned version\n",
    "# Conv2D 32, 9x9, Conv2D 64, 5x5, Conv2D 128 3x3, max pooling 7x7, dropout .25, 2x 400 Dense, dropout .3\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wilso\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        2624      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               205200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 26)                10426     \n",
      "=================================================================\n",
      "Total params: 503,770\n",
      "Trainable params: 503,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\wilso\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 104000 samples, validate on 20800 samples\n",
      "Epoch 1/17\n",
      "104000/104000 [==============================] - 22s 213us/step - loss: 1.2179 - accuracy: 0.6333 - val_loss: 0.3232 - val_accuracy: 0.8930\n",
      "Epoch 2/17\n",
      "104000/104000 [==============================] - 20s 193us/step - loss: 0.3791 - accuracy: 0.8778 - val_loss: 0.2450 - val_accuracy: 0.9184\n",
      "Epoch 3/17\n",
      "104000/104000 [==============================] - 20s 192us/step - loss: 0.2936 - accuracy: 0.9045 - val_loss: 0.2107 - val_accuracy: 0.9289\n",
      "Epoch 4/17\n",
      "104000/104000 [==============================] - 20s 194us/step - loss: 0.2529 - accuracy: 0.9159 - val_loss: 0.1990 - val_accuracy: 0.9347\n",
      "Epoch 5/17\n",
      "104000/104000 [==============================] - 20s 194us/step - loss: 0.2293 - accuracy: 0.9234 - val_loss: 0.1830 - val_accuracy: 0.9394\n",
      "Epoch 6/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.2150 - accuracy: 0.9279 - val_loss: 0.1827 - val_accuracy: 0.9399\n",
      "Epoch 7/17\n",
      "104000/104000 [==============================] - 20s 197us/step - loss: 0.2004 - accuracy: 0.9325 - val_loss: 0.1770 - val_accuracy: 0.9410\n",
      "Epoch 8/17\n",
      "104000/104000 [==============================] - 20s 197us/step - loss: 0.1913 - accuracy: 0.9346 - val_loss: 0.1695 - val_accuracy: 0.9444\n",
      "Epoch 9/17\n",
      "104000/104000 [==============================] - 20s 197us/step - loss: 0.1820 - accuracy: 0.9374 - val_loss: 0.1687 - val_accuracy: 0.9443\n",
      "Epoch 10/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.1749 - accuracy: 0.9390 - val_loss: 0.1685 - val_accuracy: 0.9456\n",
      "Epoch 11/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.1693 - accuracy: 0.9408 - val_loss: 0.1590 - val_accuracy: 0.9471\n",
      "Epoch 12/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.1665 - accuracy: 0.9416 - val_loss: 0.1615 - val_accuracy: 0.9475\n",
      "Epoch 13/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.1591 - accuracy: 0.9442 - val_loss: 0.1664 - val_accuracy: 0.9470\n",
      "Epoch 14/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.1558 - accuracy: 0.9448 - val_loss: 0.1601 - val_accuracy: 0.9476\n",
      "Epoch 15/17\n",
      "104000/104000 [==============================] - 20s 195us/step - loss: 0.1532 - accuracy: 0.9455 - val_loss: 0.1550 - val_accuracy: 0.9483\n",
      "Epoch 16/17\n",
      "104000/104000 [==============================] - 20s 197us/step - loss: 0.1479 - accuracy: 0.9469 - val_loss: 0.1534 - val_accuracy: 0.9493\n",
      "Epoch 17/17\n",
      "104000/104000 [==============================] - 20s 196us/step - loss: 0.1450 - accuracy: 0.9485 - val_loss: 0.1572 - val_accuracy: 0.9474\n",
      "Test loss: 0.1591659926763476\n",
      "Test accuracy: 0.9472596049308777\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "\n",
    "\n",
    "batch_size = 400\n",
    "num_classes = 26\n",
    "epochs = 17\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(9, 9),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(7, 7)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data = (x_cv, y_cv))\n",
    "         #, callbacks=[tensorboard_callback])\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
