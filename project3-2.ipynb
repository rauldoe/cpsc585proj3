{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# %tensorflow_version 2.x\n",
    "\n",
    "import csv\n",
    "import timeit\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "# Importing the EMNIST letters\n",
    "from scipy import io as sio\n",
    "\n",
    "# from cloud import Cloud\n",
    "from hardware import Hardware\n",
    "from utility import Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data2d:\n",
    "    def __init__(self, xData, yData):\n",
    "        self.x = xData\n",
    "        self.y = yData\n",
    "\n",
    "class InputShape:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.modified = None\n",
    "\n",
    "    def normalized(self):\n",
    "        return self.modified\n",
    "\n",
    "class LayerDescriptor:\n",
    "    def __init__(self, modelDescriptor, inputList, index, descriptionCount):\n",
    "        self.modelDescriptor = modelDescriptor\n",
    "        self.InputList = inputList\n",
    "        self.LayerType = inputList[0]\n",
    "        self.data = ','.join(inputList)\n",
    "        self.index = index\n",
    "        self.descriptionCount = descriptionCount\n",
    "        self.Activation = None\n",
    "        self.FilterCount = None\n",
    "        self.KernelSize = None\n",
    "\n",
    "    def build(self):\n",
    "        layerType = self.LayerType\n",
    "        inputList = self.InputList\n",
    "\n",
    "        if (layerType == 'conv2d'):\n",
    "            self.Activation = inputList[1]\n",
    "            self.FilterCount = int(inputList[2])\n",
    "            self.KernelSize = (int(inputList[3]), int(inputList[4]))\n",
    "\n",
    "            kernelRegularizer = LayerDescriptor.kernelRegularizer(inputList[3], inputList[4])\n",
    "            if (self.index == 0):\n",
    "                layer = Conv2D(self.FilterCount, kernel_size=self.KernelSize, activation=self.Activation, kernel_regularizer=kernelRegularizer, input_shape=self.modelDescriptor.inputShape.normalized())\n",
    "            else:\n",
    "                layer = Conv2D(self.FilterCount, kernel_size=self.KernelSize, activation=self.Activation, kernel_regularizer=kernelRegularizer)\n",
    "\n",
    "        elif (self.LayerType == 'maxpooling2d'):\n",
    "            layer = MaxPooling2D(pool_size=(int(inputList[1]), int(inputList[2])))\n",
    "\n",
    "        elif (self.LayerType == 'dropout'):\n",
    "            layer = Dropout(float(inputList[1]))\n",
    "\n",
    "        elif (self.LayerType == 'flatten'):\n",
    "            layer = Flatten()\n",
    "\n",
    "        elif (self.LayerType == 'dense'):\n",
    "            self.Activation = inputList[1]\n",
    "\n",
    "            if (self.index+1 == self.descriptionCount):\n",
    "                layer = Dense(self.modelDescriptor.numberOfClasses, activation=self.Activation)\n",
    "            else:\n",
    "                layer = Dense(int(inputList[2]), activation=self.Activation)\n",
    "        \n",
    "        return layer\n",
    "\n",
    "    @staticmethod\n",
    "    def kernelRegularizer(funcName, value):\n",
    "\n",
    "        if (funcName == 'l1'):\n",
    "            return regularizers.l1(value)\n",
    "        elif (funcName == 'l2'):\n",
    "            return regularizers.l2(value)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def create(modelDescriptor, inputList, index, descriptionCount):\n",
    "        return LayerDescriptor(modelDescriptor, inputList, index, descriptionCount)\n",
    "\n",
    "class LayerInstance:\n",
    "    def __init__(self, layer, descriptor):\n",
    "        self.layer = layer\n",
    "        self.descriptor = descriptor\n",
    "\n",
    "class ModelData:\n",
    "    def __init__(self, train, test, val, inputShape):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.val = val\n",
    "        self.inputShape = inputShape\n",
    "\n",
    "class ModelDescriptor:\n",
    "    def __init__(self, descriptorFilepath, archivePath, badPath, hyperparameters, descriptions, inputShape, numberOfClasses, threshholdAccuracy):\n",
    "        self.descriptorFilepath = descriptorFilepath\n",
    "        self.archivePath = archivePath\n",
    "        self.badPath = badPath\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.descriptions = descriptions\n",
    "        self.threshholdAccuracy = threshholdAccuracy\n",
    "\n",
    "        # processCount, epochCount, batchSize, kernelRegularizer, kernelRegularizerValue, loss, optimizer\n",
    "        i = -1\n",
    "\n",
    "        i += 1\n",
    "        self.processCount = int(hyperparameters[i])\n",
    "        i += 1\n",
    "        self.epochCount = int(hyperparameters[i])\n",
    "        i += 1\n",
    "        self.batchSize = int(hyperparameters[i])\n",
    "        i += 1\n",
    "        self.loss = hyperparameters[i]\n",
    "        i += 1\n",
    "        self.optimizer = hyperparameters[i]\n",
    "\n",
    "        self.inputShape = inputShape\n",
    "        self.numberOfClasses = numberOfClasses\n",
    "        self.hash = Utility.generateHashFromFile(descriptorFilepath)\n",
    "    \n",
    "    def archive(self):\n",
    "        shutil.move(self.descriptorFilepath,  self.archivePath)\n",
    "\n",
    "    def bad(self):\n",
    "        shutil.move(self.descriptorFilepath,  self.badPath)\n",
    "\n",
    "    @staticmethod\n",
    "    def create(descriptorFilepath, archivePath, badPath, inputShape, numberOfClasses, threshholdAccuracy):\n",
    "        descriptions = Utility.readCsv(descriptorFilepath)\n",
    "        hyperparameters = descriptions.pop(0)\n",
    "        return ModelDescriptor(descriptorFilepath, archivePath, badPath, hyperparameters, descriptions, inputShape, numberOfClasses, threshholdAccuracy)\n",
    "\n",
    "class ModelInstance:\n",
    "    def __init__(self, descriptor, model=None, layers=None, score=None):\n",
    "        self.descriptor = descriptor\n",
    "        self.model = model\n",
    "        self.layers = layers\n",
    "        self.score = score\n",
    "\n",
    "    @staticmethod\n",
    "    def loadModelData(matFilepath, modelDescriptor):\n",
    "\n",
    "        inputShape = copy.deepcopy(modelDescriptor.inputShape)\n",
    "        numberOfClasses = modelDescriptor.numberOfClasses\n",
    "\n",
    "        # https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset/53547262#53547262\n",
    "        # mat = sio.loadmat('emnist-letters.mat')\n",
    "        mat = sio.loadmat(matFilepath)\n",
    "        data = mat['dataset']\n",
    "\n",
    "        train = Data2d(data['train'][0,0]['images'][0,0], data['train'][0,0]['labels'][0,0])\n",
    "        test = Data2d(data['test'][0,0]['images'][0,0], data['test'][0,0]['labels'][0,0])\n",
    "\n",
    "        trainShape = train.x.shape[0]\n",
    "        testShape = test.x.shape[0]\n",
    "\n",
    "        val_start = trainShape - testShape\n",
    "        val = Data2d(train.x[val_start:trainShape,:], train.y[val_start:trainShape])\n",
    "        train.x = train.x[0:val_start,:]\n",
    "        train.y = train.y[0:val_start]\n",
    "\n",
    "        trainShape = train.x.shape[0]\n",
    "        testShape = test.x.shape[0]\n",
    "        valShape = val.x.shape[0]\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            train.x = train.x.reshape(trainShape, 1, inputShape.rows, inputShape.cols)\n",
    "            test.x = test.x.reshape(testShape, 1, inputShape.rows, inputShape.cols)\n",
    "            val.x = val.x.reshape(valShape, 1, inputShape.rows, inputShape.cols)\n",
    "            inputShape.modified = (1, inputShape.rows, inputShape.cols)\n",
    "        else:\n",
    "            train.x = train.x.reshape(trainShape, inputShape.rows, inputShape.cols, 1)\n",
    "            test.x = test.x.reshape(testShape, inputShape.rows, inputShape.cols, 1)\n",
    "            val.x = val.x.reshape(valShape, inputShape.rows, inputShape.cols, 1)\n",
    "            inputShape.modified = (inputShape.rows, inputShape.cols, 1)\n",
    "\n",
    "        train.x = train.x.astype('float32')\n",
    "        test.x = test.x.astype('float32')\n",
    "        train.x /= 255\n",
    "        test.x /= 255\n",
    "        print('x_train shape:', train.x.shape)\n",
    "        print(train.x.shape[0], 'train samples')\n",
    "        print(test.x.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        train.y = tf.keras.utils.to_categorical(train.y - 1, numberOfClasses, dtype='float32')\n",
    "        test.y = tf.keras.utils.to_categorical(test.y - 1, numberOfClasses, dtype='float32')\n",
    "\n",
    "        val.y = tf.keras.utils.to_categorical(val.y - 1, numberOfClasses, dtype='float32')\n",
    "\n",
    "        return ModelData(train, test, val, inputShape)\n",
    "\n",
    "    @staticmethod\n",
    "    def buildLayers(modelInstance, descriptions):\n",
    "        layers = []\n",
    "\n",
    "        descriptionCount = len(descriptions)\n",
    "        for i in range(descriptionCount):\n",
    "            desc = descriptions[i]\n",
    "            layerDescriptor = LayerDescriptor.create(modelInstance.descriptor, desc, i, descriptionCount)\n",
    "            layers.append(LayerInstance(layerDescriptor.build(), layerDescriptor))\n",
    "\n",
    "        modelInstance.layers = layers\n",
    "\n",
    "        return modelInstance\n",
    "\n",
    "    @staticmethod\n",
    "    def buildModel(modelInstance, layerInstances):\n",
    "        model = Sequential()\n",
    "\n",
    "        for i in range(len(layerInstances)):\n",
    "            layer = layerInstances[i].layer\n",
    "            layerDescriptor = layerInstances[i].descriptor\n",
    "            print(\"i: %d type: %s data: %s\"%(i, layerDescriptor.LayerType, layerDescriptor.data)) \n",
    "            model.add(layer)\n",
    "            print(\"i: %d added\"%(i)) \n",
    "\n",
    "        model.compile(loss=modelInstance.descriptor.loss,\n",
    "                    optimizer=modelInstance.descriptor.optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        modelInstance.model = model\n",
    "\n",
    "        return modelInstance\n",
    "\n",
    "    @staticmethod\n",
    "    def create(descriptor):\n",
    "        return ModelInstance(descriptor)\n",
    "\n",
    "class ScoreBoard:\n",
    "    def __init__(self, scoreFilepath):\n",
    "        self.scoreFilepath = scoreFilepath\n",
    "        self.scoreList = Utility.readCsv(scoreFilepath)\n",
    "    \n",
    "    def isDone(self, modelDescriptor):\n",
    "        itemList = self.getItems(modelDescriptor.hash)\n",
    "        if (len(itemList) > 0):\n",
    "            scoreObj = itemList[0]\n",
    "            \n",
    "            accuracy = float(scoreObj[1])\n",
    "            return ((accuracy < modelDescriptor.threshholdAccuracy) or (int(scoreObj[4]) >= modelDescriptor.processCount))\n",
    "\n",
    "        return False\n",
    "\n",
    "    def isInList(self, identifier):\n",
    "        return True in (i[0] == identifier for i in self.scoreList)\n",
    "    \n",
    "    def getItems(self, identifier):\n",
    "        return list(filter(lambda x: x[0] == identifier, self.scoreList))\n",
    "\n",
    "    def record(self, modelDescriptor, score):\n",
    "\n",
    "        scoreObj = None\n",
    "        count = 0\n",
    "        itemList = self.getItems(modelDescriptor.hash)\n",
    "        if (len(itemList) > 0):\n",
    "            scoreObj = itemList[0]\n",
    "            count = int(scoreObj[4]) + 1\n",
    "            scoreObj[4] = count\n",
    "            if (score[1] > float(scoreObj[1])):\n",
    "                scoreObj[1] = score[1]\n",
    "                scoreObj[2] = score[0]\n",
    "        else:\n",
    "            count = 1\n",
    "            scoreObj = [modelDescriptor.hash, score[1], score[0], modelDescriptor.descriptorFilepath, count]\n",
    "            self.scoreList.append(scoreObj)\n",
    "        \n",
    "        self.scoreList.sort(key=lambda i:float(i[1]), reverse=True)\n",
    "\n",
    "        newList = copy.copy(self.scoreList)\n",
    "        newList.insert(0, ['#hash', 'score1', 'score0', 'filePath', 'count'])\n",
    "        Utility.writeCsv(self.scoreFilepath, newList)\n",
    "\n",
    "        if (self.isDone(modelDescriptor)):\n",
    "            modelDescriptor.archive()\n",
    "\n",
    "def processNN(matFilepath, modelInstance):\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "    earlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', min_delta=0.0001, patience=5, verbose=0, mode='auto',\n",
    "            baseline=None, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    modelData = ModelInstance.loadModelData(matFilepath, modelInstance.descriptor)\n",
    "\n",
    "    modelInstance.descriptor.inputShape = modelData.inputShape\n",
    "\n",
    "    modelInstance = ModelInstance.buildLayers(modelInstance, modelInstance.descriptor.descriptions)\n",
    "\n",
    "    try:\n",
    "        modelInstance = ModelInstance.buildModel(modelInstance, modelInstance.layers)\n",
    "\n",
    "        modelInstance.model.fit(modelData.train.x, modelData.train.y,\n",
    "                            batch_size=modelInstance.descriptor.batchSize,\n",
    "                            epochs=modelInstance.descriptor.epochCount, callbacks=[earlyStop],\n",
    "                            validation_data=(modelData.val.x, modelData.val.y)\n",
    "                            )\n",
    "        modelInstance.model.summary()\n",
    "        score = modelInstance.model.evaluate(modelData.test.x, modelData.test.y, verbose=0)\n",
    "\n",
    "        # score = [54.34, 98.55]\n",
    "        modelInstance.score = score\n",
    "\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "\n",
    "    return modelInstance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (104000, 28, 28, 1)\n",
      "104000 train samples\n",
      "20800 test samples\n",
      "i: 0 type: conv2d data: conv2d,relu,32,3,3,l1,0.0002\n",
      "i: 0 added\n",
      "i: 1 type: conv2d data: conv2d,relu,64,3,3,,\n",
      "i: 1 added\n",
      "i: 2 type: maxpooling2d data: maxpooling2d,2,2\n",
      "i: 2 added\n",
      "i: 3 type: dropout data: dropout,0.25\n",
      "i: 3 added\n",
      "i: 4 type: flatten data: flatten\n",
      "i: 4 added\n",
      "i: 5 type: dense data: dense,relu,128\n",
      "i: 5 added\n",
      "i: 6 type: dense data: dense,softmax\n",
      "i: 6 added\n",
      "Train on 104000 samples, validate on 20800 samples\n",
      "Epoch 1/1000\n",
      "104000/104000 [==============================] - 66s 630us/step - loss: 1.4466 - accuracy: 0.5858 - val_loss: 108.4899 - val_accuracy: 0.7450\n",
      "Epoch 2/1000\n",
      "104000/104000 [==============================] - 66s 632us/step - loss: 0.6354 - accuracy: 0.8069 - val_loss: 71.5717 - val_accuracy: 0.8459\n",
      "Epoch 3/1000\n",
      "104000/104000 [==============================] - 66s 636us/step - loss: 0.4576 - accuracy: 0.8580 - val_loss: 52.4856 - val_accuracy: 0.8844\n",
      "Epoch 4/1000\n",
      "104000/104000 [==============================] - 66s 639us/step - loss: 0.3636 - accuracy: 0.8842 - val_loss: 57.9990 - val_accuracy: 0.8825\n",
      "Epoch 5/1000\n",
      "104000/104000 [==============================] - 66s 635us/step - loss: 0.3134 - accuracy: 0.8996 - val_loss: 63.7964 - val_accuracy: 0.8762\n",
      "Epoch 6/1000\n",
      "104000/104000 [==============================] - 70s 674us/step - loss: 0.2740 - accuracy: 0.9104 - val_loss: 43.2571 - val_accuracy: 0.9127\n",
      "Epoch 7/1000\n",
      "104000/104000 [==============================] - 66s 636us/step - loss: 0.2415 - accuracy: 0.9206 - val_loss: 47.7545 - val_accuracy: 0.9073\n",
      "Epoch 8/1000\n",
      "104000/104000 [==============================] - 66s 639us/step - loss: 0.2197 - accuracy: 0.9261 - val_loss: 44.7893 - val_accuracy: 0.9146\n",
      "Epoch 9/1000\n",
      "104000/104000 [==============================] - 66s 637us/step - loss: 0.1985 - accuracy: 0.9333 - val_loss: 54.4434 - val_accuracy: 0.9042\n",
      "Epoch 10/1000\n",
      "104000/104000 [==============================] - 66s 633us/step - loss: 0.1824 - accuracy: 0.9373 - val_loss: 45.5887 - val_accuracy: 0.9189\n",
      "Epoch 11/1000\n",
      "104000/104000 [==============================] - 66s 632us/step - loss: 0.1649 - accuracy: 0.9427 - val_loss: 49.4023 - val_accuracy: 0.9155\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 1,201,946\n",
      "Trainable params: 1,201,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 0.29316699828200327\n",
      "Test accuracy: 0.910528838634491\n"
     ]
    }
   ],
   "source": [
    "# Batch size of 128 had about 90.4% accuracy.\n",
    "# Thus, a batch size of 1000 was used where accuracy was about 91.5%. \n",
    "# Signifigantly higher batch sizes also decreased test accuracy.\n",
    "# A batch size of 104,000 led to an accuracy of about \n",
    "# batchSize = 2000\n",
    "# epochCount = 1000 #There is early stopping, so it won't reach 1000 epochs. This needs to be high.\n",
    "\n",
    "inputShape = InputShape(28, 28)\n",
    "numberOfClasses = 26\n",
    "\n",
    "matFilepath = './emnist-letters.mat'\n",
    "\n",
    "scoreFilepath = './models/results/scoreboard.csv'\n",
    "\n",
    "modelDir = './models/'\n",
    "archivePath = './models/archive/'\n",
    "badPath = './models/archive/bad/'\n",
    "threshholdAccuracy = 0.90\n",
    "\n",
    "scoreBoard = ScoreBoard(scoreFilepath)\n",
    "modelFiles = Utility.getFiles(modelDir)\n",
    "for modelFile in modelFiles:\n",
    "    descriptorPath = '%s%s'%(modelDir, modelFile)\n",
    "    modelDescriptor = ModelDescriptor.create(descriptorPath, archivePath, badPath, inputShape, numberOfClasses, threshholdAccuracy)\n",
    "\n",
    "    if (scoreBoard.isDone(modelDescriptor)):\n",
    "        modelDescriptor.archive()\n",
    "    else:\n",
    "        modelInstance = ModelInstance.create(modelDescriptor)\n",
    "\n",
    "        modelInstance = processNN(matFilepath, modelInstance)\n",
    "\n",
    "        if (modelInstance.score == None):\n",
    "            modelDescriptor.bad()\n",
    "        else:\n",
    "            scoreBoard.record(modelDescriptor, modelInstance.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
